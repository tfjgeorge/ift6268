\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}


\title{Texture sequence generation using convolutional networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Thomas George\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Université de Montréal\\
  \texttt{tfjgeorge@gmail.com} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  TODO
  
  An emphasize is put on understanding the different issues that arise when implementing and training texture generator networks.
\end{abstract}

\section{Introduction}

Automatic texture generation is a difficult problem that has many applications in artwork creation. Traditional tools use a base texture image that is copied and somehow modified to create what looks like a new texture. A recent paper by Gatys et al. \cite{gatys2015neural} \cite{gatys2015texture} proposed a radically new approach: use convolutional neural networks and an optimization procedure that allowed for creating very realistic new images of a given texture. This work and a follow-up by the same team that presented a technique to merge the style of an artwork with the content of a photograph led to the creation of the company, emphasizing the potential success for automatic artwork generation.

This work was done as the term project for the course IFT6268: Vision given by Pr. Roland Memisevitch at Université de Montréal during Winter 2016.

\section{Texture synthesis}

In this section we describe the basic ideas behind the use of convolutional networks as tool to generate new images.

\subsection{Using convolutional networks as feature descriptors}

Following the success of convolutional neural networks for solving image classification tasks, a lot of pretrained networks for different architectures have been made publicly available. This convolutional networks are powerful feature extractors that start from a raw pixel image and sequentially build new representations at each layer that are then used by a standard logistic regression as a top layer to predict a class from a given set.

While the representations in the first layers are low level representations that describe the edges orientation and the colors of the images, deeper representations represent more abstract objects such as complex shapes and arrangements of colors. Thus a texture synthesis task can be thought as finding an image with a similar texture to a base image, the similarity being measured using the metrics based on this representations using the metrics described in the following paragraphs.

\subsection{Formulating texture generation as an optimization problem}

TODO

In the following we will refer to the network used to define the lost function as the descriptor network.

\subsection{Choosing layer to best describe a texture}

A problem arising with this technique is that we have to choose which layer best describe a texture.

Proposed solution : for each layer compute distance between gram matrix for layer for image and flipped image

The network we use to extract our descriptors was pre trained on ImageNet. TODO

\subsection{Texture synthesis strategies}

Gatys et al. proposed a strategy that consists in starting from a random image and iterate gradient descent steps on the value of the pixels to find a minimum of the loss function. This gives results that are visually appealing and is quite simple to train using a framework where getting the gradients by back-propagation is easy to do such as Theano \cite{bastien2012theano}. A caveat with this procedure is that it takes a full learning run each time you want to generate a new texture.

A follow-up was proposed by  \citeauthor{2016arXiv160303417U}, who describe a generator network that uses random noise images as input to directly output a texture image. The texture generation only needs a feed forward pass which allows to generate images in near real time. This network is described in the following section.

\section{Generator network}

In this section we describe the feed forward generator network that we use, some heuristics for choosing the hyperparameters and the issues encountered while training this networks.

\subsection{Architecture}

We use the generator network described in 

\subsection{Choosing the number of filters for each resolution}

Choosing the number of filters used at each resolution is also a difficult problem with no real answer. As a heuristic, we consider the following: in our descriptor network, we made a choice for which layers to choose to best describe a texture. The choice of filters must be done with the idea that at each layer chosen in the descriptor network corresponds a spatial resolution, i.e. each time we do subsampling the spatial resolution is divided by 2. We should add more filters for the corresponding resolution in the generator network.

\subsection{Training}

Training this generator network is challenging. Here are the setup that worked best in our experiments.

We performed stochastic gradient descent on mini batches of size 5 up to 16 depending on if the model could fit on the GPU memory. We used Adam\cite{kingma2014adam} with a learning rate of $1e-3$, with early stopping. At each early stopping trigger we divided the learning rate by 5 then started a new learning procedure for a total of 3 runs. Training such a model takes about 2h on a Titan Black GPU.

Implementing the model was done using the framework Blocks \cite{van2015blocks} on top of Theano. We used a pretrained VGGNet network available on the Lasagne Recipe repository.

\subsection{Issues}
\subsubsection{Overfitting}

A global minimum to this optimization problem is obvious : take the original texture image and you get a zero loss function. This is not desired, as we want the generator to be able to generate new texture images, not to reproduce an existing image. The optimization procedure does not seem to find this kind of minimum however.

But an issue that arises is that the biases in the convolution layers learn a local optimum. This usually happens along the edges of the image and we hypothesize this is an effect of the border convolutions being zero-padded (\label{fig:overfit}). A solution could be to add a regularization term that penalizes images to be too close in a mini batch but this has not being explored due to lack of time. Another simpler solution could be to regularize the norm of the biases.

To counter this issue we simply did not use bias, even if it allowed for faster initial convergence.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{overfit1}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{overfit2}
    \end{subfigure}
   
    \caption{Two images generated using two different noise images using a trained network that has learned a good minimum for the biases. Notice how both images are similar around the edges and especially in the corners, while being really different in the middle.}\label{fig:overfit}
\end{figure}

\subsubsection{Capacity}

number of 

\subsubsection{Memory}

\section{Video textures}

\subsection{Motivation}
Once we have a base texture generator, we can think of it as a way to sample from a texture manifold lying in the space of pixels. The random noise images given as input of the generators can be considered as latent variables that determine where to pick the sample along that manifold. By interpolating linearly between the pixels values for two random images we can get a sequence of images with each consecutive image being very similar to the previous one, and in the meantime being in the manifold of images that have a similar texture to the target texture. This idea motivates building sequences of frames that transform a texture image into a new texture image.

We did not intend to generate realistic videos. As a simple example, think of the transformation between two textures representing red peppers. We do not see red pepper turn into air then back to red pepper in the real world. However we find the results visually appealing. Note that for some example (e.g. water) the video could look somehow realistic.

\subsection{Procedure}

This section describes the procedure used to generate sequences of transformations between two texture images. We first sample two sets of random images of required sizes (256, 128, 64, 32, 16). We then interpolate between these two images, so that the random images given as input to the generator network are given by:
$$x^{(i)}(t) = t \, x^{(i)}_1 + (1-t) \,  x^{(i)}_2 \, , t \in [0,1]$$

We then construct a sequence with the output images. The frame rate is 30 per second and we increase $t$ by $0.01$ at each time step.

\subsection{Experiments and discussion}

We have mainly experimented using a red pepper texture as a base. The generated videos can be watched at \url{http://youtu.be/ghFM79hpPBw}. 

A further experiment consisted in varying only one random image at a time in the generator, keeping all other fixed. Interestingly, some random images are not used by the generator and varying them leads to no visual effect:

\begin{center}
  \begin{tabular}{  c | l | l }
  Varying random image & Link & Comment\\
    \hline
    256 x 256 & \url{http://youtu.be/OJ_96ucypr4} & No effect\\ 
    128 x 128 & \url{http://youtu.be/1kVjfbPmsTU} & No effect\\ 
    64 x 64 & \url{http://youtu.be/j2SFynUCSOU} & Very light effect \\
    32 x 32 & \url{http://youtu.be/J_f5wlW0xEg} & Light effect\\
    16 x 16 & \url{http://youtu.be/BXvIU_ZIyNs} & Strong effect\\
  \end{tabular}
\end{center}


\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
